{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2301874-9b87-4ff2-b488-2607e53f3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import re # regular expressions\n",
    "import pandas as pd # pandase\n",
    "import glob\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster('spark://spark:7077')\n",
    "conf.set('spark.executor.memory', '512m')\n",
    "conf.set('spark.executor.cores', '1')\n",
    "conf.set('spark.cores.max', '1')\n",
    "\n",
    "# creating the spark sql session\n",
    "spark = pyspark.sql.SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9011c856-337b-43c1-b601-41c1f14ee974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data/NASA_access_log_Aug95.gz', '/data/NASA_access_log_Jul95.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next step is to read the NASA datasets which are having the extension of .gz\n",
    "\n",
    "raw_data_files = glob.glob('/data/*.gz')\n",
    "raw_data_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "777b6fef-9557-4add-b2bd-f4d2c269571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the files data using spark read which converts it to DataFrame\n",
    "base_dataframe = spark.read.text(raw_data_files)\n",
    "\n",
    "# a single string column showing value as string\n",
    "base_dataframe.printSchema()\n",
    "\n",
    "# confirm the type for the read data = DataFrame\n",
    "type(base_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a901a0-99c5-4ef5-a71e-fffbc5c4b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0                    |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0        |\n",
      "|205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985             |\n",
      "|d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                               |\n",
      "|129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# counting the data \n",
    "base_dataframe.count()\n",
    "\n",
    "# printing some log data\n",
    "base_dataframe.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec860e1-1bf1-4615-9688-aae6b523a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3461613, 1)\n",
      "['6245', '3985', '4085', '0', '4179', '0', '0', '3985', '3985', '7074', '40310', '786', '1204', '40310', '786', '1204', '0', '1713', '3977', '34029']\n"
     ]
    }
   ],
   "source": [
    "# Data wrangling - data cleaning, parsing to extract structured attributes with meaningful information\n",
    "# from each of the log message.\n",
    "\n",
    "# The common log format according to https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format is the following\n",
    "# remotehost rfc931 authuser [date] \"request\" status bytes\n",
    "\n",
    "# remotehost = Remote hostname (or IP number if DNS hostname is not available, or if DNSLookup is Off.\n",
    "# fc931 = The remote logname of the user.\n",
    "# authUser = The username as which the user has authenticated himself.\n",
    "# [date] = Date and time of the request.\n",
    "# request = The request line exactly as it came from the client.\n",
    "# status = The HTTP status code returned to the client.\n",
    "# bytes = The content-length of the document transferred.\n",
    "\n",
    "# parse the semi-structured log data into individual columns\n",
    "# regexp_extract() function to do the parsing. \n",
    "# This function matches a column against a regular expression \n",
    "# with one or more capture groups and allows you to extract one \n",
    "# of the matched groups. Weâ€™ll use one regular expression for each field we wish to extract.\n",
    "\n",
    "# total number of logs we are currently deadling with\n",
    "# 3.46 million log messages \n",
    "print((base_dataframe.count(), len(base_dataframe.columns)))\n",
    "\n",
    "# some sample logs\n",
    "sample_logs = [item['value'] for item in base_dataframe.take(20)]\n",
    "sample_logs\n",
    "\n",
    "# regular expression for extracing the host information from the logs\n",
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "# use this to understand the regular expressions = https://regexr.com/\n",
    "\n",
    "hosts = [re.search(host_pattern, item).group(1)\n",
    "           if re.search(host_pattern, item)\n",
    "           else 'no match'\n",
    "           for item in sample_logs]\n",
    "#print(hosts)\n",
    "\n",
    "# regular expression for extracting the timestamp or [date] feature from the log\n",
    "ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "\n",
    "timestamps = [re.search(ts_pattern, item).group(1) for item in sample_logs]\n",
    "\n",
    "#print(timestamps)\n",
    "\n",
    "# regular expression for extracting the HTTP Method, URL and Protocol from the logs\n",
    "method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "method_uri_protocol = [re.search(method_uri_protocol_pattern, item).groups()\n",
    "               if re.search(method_uri_protocol_pattern, item)\n",
    "               else 'no match'\n",
    "              for item in sample_logs]\n",
    "\n",
    "#print(method_uri_protocol)\n",
    "\n",
    "# regular expression for extracting HTTP status codes for each log request\n",
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "status = [re.search(status_pattern, item).group(1) for item in sample_logs]\n",
    "\n",
    "#print(status)\n",
    "\n",
    "\n",
    "# regular expression for getting the response content size\n",
    "content_size_pattern = r'\\s(\\d+)$'\n",
    "content_size = [re.search(content_size_pattern, item).group(1) for item in sample_logs]\n",
    "\n",
    "print(content_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909d7895-89b6-423a-b2bf-4a2f940cedf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|                host|           timestamp|method|            endpoint|protocol|status|content_size|\n",
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|        199.72.81.55|01/Jul/1995:00:00...|   GET|    /history/apollo/|HTTP/1.0|   200|        6245|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        4085|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   304|           0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        4179|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/images/NASA-logo...|HTTP/1.0|   304|           0|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   200|           0|\n",
      "|     205.212.115.106|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   200|        3985|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|   GET|                   /|HTTP/1.0|   200|        7074|\n",
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "(3461613, 7)\n"
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "\n",
    "logs_dataframe = base_dataframe.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                         regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                         regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "logs_dataframe.show(10, truncate=True)\n",
    "\n",
    "print((logs_dataframe.count(), len(logs_dataframe.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c940fc79-45b9-4464-a1b5-7029d5934f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33905"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for null rows in the original dataframe\n",
    "base_dataframe.filter(base_dataframe['value'].isNull()).count()\n",
    "\n",
    "null_rows_dataframe = logs_dataframe.filter(logs_dataframe['host'].isNull()| \n",
    "                             logs_dataframe['timestamp'].isNull() | \n",
    "                             logs_dataframe['method'].isNull() |\n",
    "                             logs_dataframe['endpoint'].isNull() |\n",
    "                             logs_dataframe['status'].isNull() |\n",
    "                             logs_dataframe['content_size'].isNull()|\n",
    "                             logs_dataframe['protocol'].isNull())\n",
    "null_rows_dataframe.count()\n",
    "\n",
    "# we have almost 33k null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41a6c567-3d26-4ff0-b894-2612ac9cdb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     1|       33905|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To find out which columns have the null values\n",
    "def count_null(col_name):\n",
    "    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "# Build up a list of column expressions, one per column.\n",
    "exprs = [count_null(col_name) for col_name in logs_dataframe.columns]\n",
    "\n",
    "# Run the aggregation. The *exprs converts the list of expressions into\n",
    "# variable function arguments.\n",
    "logs_dataframe.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2c5424-77c2-4a07-a111-9100a2666603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling nulls in HTTP status\n",
    "\n",
    "null_status_dataframe = base_dataframe.filter(~base_dataframe['value'].rlike(r'\\s(\\d{3})\\s'))\n",
    "null_status_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15deec6-799a-4fda-9c97-014d4a9bcff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|value   |\n",
      "+--------+\n",
      "|alyssa.p|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Letâ€™s look at what this bad record looks like!\n",
    "null_status_dataframe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60f478b4-e4be-4fe2-801e-013d0608d615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|    |         |      |        |        |null  |null        |\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looks like a record with a lot of missing information! Letâ€™s pass this through our log data parsing pipeline.\n",
    "\n",
    "bad_status_dataframe = null_status_dataframe.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                                      regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                                      regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                                      regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "bad_status_dataframe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a6c9e6-2959-4d10-902c-c2bf091ca7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     0|       33904|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looks like the record itself is an incomplete record with no useful information, \n",
    "# the best option would be to drop this record as follows!\n",
    "\n",
    "logs_dataframe = logs_dataframe[logs_dataframe['status'].isNotNull()]\n",
    "exprs = [count_null(col_name) for col_name in logs_dataframe.columns]\n",
    "logs_dataframe.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d0e3a8f-6210-4a78-9661-d4f17b654c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33905"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handling nulls in HTTP content size\n",
    "\n",
    "null_content_size_dataframe = base_dataframe.filter(~base_dataframe['value'].rlike(r'\\s\\d+$'))\n",
    "null_content_size_dataframe.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7855c2e0-7f66-4809-87c8-705307b6c7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='dd15-062.compuserve.com - - [01/Jul/1995:00:01:12 -0400] \"GET /news/sci.space.shuttle/archive/sci-space-shuttle-22-apr-1995-40.txt HTTP/1.0\" 404 -'),\n",
       " Row(value='dynip42.efn.org - - [01/Jul/1995:00:02:14 -0400] \"GET /software HTTP/1.0\" 302 -'),\n",
       " Row(value='ix-or10-06.ix.netcom.com - - [01/Jul/1995:00:02:40 -0400] \"GET /software/winvn HTTP/1.0\" 302 -'),\n",
       " Row(value='ix-or10-06.ix.netcom.com - - [01/Jul/1995:00:03:24 -0400] \"GET /software HTTP/1.0\" 302 -'),\n",
       " Row(value='link097.txdirect.net - - [01/Jul/1995:00:05:06 -0400] \"GET /shuttle HTTP/1.0\" 302 -'),\n",
       " Row(value='ix-war-mi1-20.ix.netcom.com - - [01/Jul/1995:00:05:13 -0400] \"GET /shuttle/missions/sts-78/news HTTP/1.0\" 302 -'),\n",
       " Row(value='ix-war-mi1-20.ix.netcom.com - - [01/Jul/1995:00:05:58 -0400] \"GET /shuttle/missions/sts-72/news HTTP/1.0\" 302 -'),\n",
       " Row(value='netport-27.iu.net - - [01/Jul/1995:00:10:19 -0400] \"GET /pub/winvn/readme.txt HTTP/1.0\" 404 -'),\n",
       " Row(value='netport-27.iu.net - - [01/Jul/1995:00:10:28 -0400] \"GET /pub/winvn/readme.txt HTTP/1.0\" 404 -'),\n",
       " Row(value='dynip38.efn.org - - [01/Jul/1995:00:10:50 -0400] \"GET /software HTTP/1.0\" 302 -')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_content_size_dataframe.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9b547f-1936-400b-9d9e-040c54ab109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     0|           0|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It is quite evident that the bad raw data records correspond to error responses, \n",
    "# where no content was sent back and the server emitted a â€œ-\" for the content_size field.\n",
    "\n",
    "# Since we donâ€™t want to discard those rows from our analysis, letâ€™s impute or fill them to 0.\n",
    "\n",
    "# Fix the rows with null content_size\n",
    "logs_dataframe = logs_dataframe.na.fill({'content_size': 0})\n",
    "exprs = [count_null(col_name) for col_name in logs_dataframe.columns]\n",
    "logs_dataframe.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94f49d8a-c06d-47ef-ac90-4809334ae406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning is done\n",
    "\n",
    "# Handling Temporal Fields (Timestamp)\n",
    "# Now that we have a clean, parsed DataFrame, we have to parse the \n",
    "# timestamp field into an actual timestamp. The Common Log Format time \n",
    "# is somewhat non-standard. A User-Defined Function (UDF) is the most \n",
    "# straightforward way to parse it.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "month_map = {\n",
    "  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "def parse_clf_time(text):\n",
    "    \"\"\" Convert Common Log time format into a Python datetime object\n",
    "    Args:\n",
    "        text (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n",
    "    Returns:\n",
    "        a string suitable for passing to CAST('timestamp')\n",
    "    \"\"\"\n",
    "    # NOTE: We're ignoring the time zones here, might need to be handled depending on the problem you are solving\n",
    "    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n",
    "      int(text[7:11]),\n",
    "      month_map[text[3:6]],\n",
    "      int(text[0:2]),\n",
    "      int(text[12:14]),\n",
    "      int(text[15:17]),\n",
    "      int(text[18:20])\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c317fc0e-0028-4fd7-aa04-b348388131dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------+------+------------+-------------------+\n",
      "|                host|method|            endpoint|protocol|status|content_size|               time|\n",
      "+--------------------+------+--------------------+--------+------+------------+-------------------+\n",
      "|        199.72.81.55|   GET|    /history/apollo/|HTTP/1.0|   200|        6245|1995-07-01 00:00:01|\n",
      "|unicomp6.unicomp.net|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|1995-07-01 00:00:06|\n",
      "|      199.120.110.21|   GET|/shuttle/missions...|HTTP/1.0|   200|        4085|1995-07-01 00:00:09|\n",
      "|  burger.letters.com|   GET|/shuttle/countdow...|HTTP/1.0|   304|           0|1995-07-01 00:00:11|\n",
      "|      199.120.110.21|   GET|/shuttle/missions...|HTTP/1.0|   200|        4179|1995-07-01 00:00:11|\n",
      "|  burger.letters.com|   GET|/images/NASA-logo...|HTTP/1.0|   304|           0|1995-07-01 00:00:12|\n",
      "|  burger.letters.com|   GET|/shuttle/countdow...|HTTP/1.0|   200|           0|1995-07-01 00:00:12|\n",
      "|     205.212.115.106|   GET|/shuttle/countdow...|HTTP/1.0|   200|        3985|1995-07-01 00:00:12|\n",
      "|         d104.aa.net|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|1995-07-01 00:00:13|\n",
      "|      129.94.144.152|   GET|                   /|HTTP/1.0|   200|        7074|1995-07-01 00:00:13|\n",
      "+--------------------+------+--------------------+--------+------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_parse_time = udf(parse_clf_time)\n",
    "\n",
    "logs_dataframe = (logs_dataframe.select('*', udf_parse_time(logs_dataframe['timestamp'])\n",
    "                                        .cast('timestamp').alias('time')).drop('timestamp'))\n",
    "                  \n",
    "                  \n",
    "\n",
    "logs_dataframe.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04b039d5-67a1-420b-96f4-91fd77d69292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- content_size: integer (nullable = false)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[host: string, method: string, endpoint: string, protocol: string, status: int, content_size: int, time: timestamp]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_dataframe.printSchema()\n",
    "\n",
    "logs_dataframe.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e8269fd-f61e-417f-931a-97df538212b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema is fine, times are according to the standard\n",
    "# we can do now the analysis part for the project\n",
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d0d765a-c8a4-43b2-b082-3fdb76ae97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Size Statistics\n",
    "# weâ€™d like to know what are the average, minimum, and maximum content sizes.\n",
    "# The .describe() function returns the count, mean, stddev, min, and max of a given column.\n",
    "\n",
    "#content_size_summary_dataframe = logs_dataframe.describe(['content_size'])\n",
    "#content_size_summary_dataframe.toPandas()\n",
    "\n",
    "#content_size_summary_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fd420-c768-494f-9f7a-f06ef8607e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL functions.\n",
    "\n",
    "from pyspark.sql import functions as SparkSqlFunctions\n",
    "\n",
    "content_size_summary_dataframe = logs_dataframe.agg(SparkSqlFunctions.min(logs_dataframe['content_size']).alias('min_content_size'),\n",
    "             SparkSqlFunctions.max(logs_dataframe['content_size']).alias('max_content_size'),\n",
    "             SparkSqlFunctions.mean(logs_dataframe['content_size']).alias('mean_content_size'),\n",
    "             SparkSqlFunctions.stddev(logs_dataframe['content_size']).alias('std_content_size'),\n",
    "             SparkSqlFunctions.count(logs_dataframe['content_size']).alias('count_content_size'))\n",
    "        \n",
    "content_size_summary_dataframe = content_size_summary_dataframe.toPandas()\n",
    "\n",
    "content_size_summary_dataframe.to_json('analysis/content_size_analysis.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6847f-ba1b-4085-9f1f-2542a2ae16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP Status Code Analysis\n",
    "# We want to know which status code values appear in the data and how many times\n",
    "\n",
    "status_freq_dataframe = (logs_dataframe\n",
    "                     .groupBy('status')\n",
    "                     .count()\n",
    "                     .sort('status')\n",
    "                     .cache())\n",
    "print('Total distinct HTTP Status Codes:', status_freq_dataframe.count())\n",
    "\n",
    "status_freq_pd_dataframe = (status_freq_dataframe\n",
    "                         .toPandas()\n",
    "                         .sort_values(by=['count'],\n",
    "                                      ascending=False))\n",
    "status_freq_pd_dataframe\n",
    "status_freq_pd_dataframe.to_json('analysis/status_codes_frequency_analysis.json')\n",
    "\n",
    "status_log_freq_dataframe = status_freq_dataframe.withColumn('log(count)', \n",
    "                                        SparkSqlFunctions.log(status_freq_dataframe['count']))\n",
    "status_log_freq_dataframe.show()\n",
    "\n",
    "status_log_freq_dataframe = status_log_freq_dataframe.toPandas()\n",
    "\n",
    "status_log_freq_dataframe.to_json('analysis/status_logs_frequency_analysis.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111549d9-89a1-49b7-889f-eccec122307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Frequent Hosts\n",
    "# the count of total accesses by each host and then sort by the \n",
    "# counts and display only the top ten most frequent hosts.\n",
    "\n",
    "host_sum_dataframe =(logs_dataframe\n",
    "               .groupBy('host')\n",
    "               .count()\n",
    "               .sort('count', ascending=False).limit(10))\n",
    "\n",
    "host_sum_dataframe.show(truncate=False)\n",
    "\n",
    "host_sum_dataframe = host_sum_dataframe.toPandas()\n",
    "\n",
    "host_sum_dataframe.to_json('analysis/frequenct_host_analysis.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c197318-11e0-4bf8-aa08-86fc6ae4e414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Top 20 Frequent EndPoints\n",
    "\n",
    "paths_dataframe = (logs_dataframe\n",
    "            .groupBy('endpoint')\n",
    "            .count()\n",
    "            .sort('count', ascending=False).limit(20))\n",
    "\n",
    "paths_pd_dataframe = paths_dataframe.toPandas()\n",
    "\n",
    "paths_pd_dataframe.to_json('analysis/20_frequent_endpoints_analysis.json')\n",
    "paths_pd_dataframe  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068df57b-3ba5-4582-882a-3b25bd0092f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Ten Error Endpoints\n",
    "# What are the top ten endpoints requested which did not have return \n",
    "# code 200 (HTTP Status OK)? We create a sorted list containing the \n",
    "# endpoints and the number of times that they were accessed with a \n",
    "# non-200 return code and show the top ten.\n",
    "\n",
    "error_endpoints_dataframe = (logs_dataframe\n",
    "               .filter(logs_dataframe['status'] != 200))\n",
    "\n",
    "error_endpoints_freq_dataframe = (error_endpoints_dataframe\n",
    "                               .groupBy('endpoint')\n",
    "                               .count()\n",
    "                               .sort('count', ascending=False)\n",
    "                               .limit(10)\n",
    "                          )\n",
    "error_endpoints_freq_dataframe.show(truncate=False) \n",
    "\n",
    "error_endpoints_freq_dataframe = error_endpoints_freq_dataframe.toPandas()\n",
    "\n",
    "error_endpoints_freq_dataframe.to_json('analysis/10_error_endpoints_analysis.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec08ee-87ee-4c01-bd66-d898289642f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of Unique Hosts\n",
    "# What were the total number of unique hosts who visited the NASA website in these two months?\n",
    "\n",
    "unique_host_count = (logs_dataframe\n",
    "                     .select('host')\n",
    "                     .distinct()\n",
    "                     .count())\n",
    "unique_host_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0619606-a6eb-4587-8bdb-d8851dec37c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Unique Daily Hosts\n",
    "\n",
    "host_per_day_dataframe = logs_dataframe.select(logs_dataframe.host, \n",
    "                             SparkSqlFunctions.dayofmonth('time').alias('day'))\n",
    "\n",
    "host_per_day_dataframe.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec11e0-6999-48c4-b6d7-fb78f6dbd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# host_per_day_distinct_dataframe : This DataFrame has the same columns as \n",
    "# host_per_day_dataframe, but with duplicate (day, host) rows removed.\n",
    "\n",
    "host_per_day_distinct_dataframe = (host_per_day_dataframe\n",
    "                          .dropDuplicates())\n",
    "host_per_day_distinct_dataframe.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fee79-67f4-4b09-901a-a0607adb74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "\n",
    "daily_hosts_dataframe = (host_per_day_distinct_dataframe\n",
    "                     .groupBy('day')\n",
    "                     .count()\n",
    "                     .sort(\"day\"))\n",
    "\n",
    "daily_hosts_dataframe = daily_hosts_dataframe.toPandas()\n",
    "\n",
    "daily_hosts_dataframe\n",
    "\n",
    "daily_hosts_dataframe.to_json('analysis/daily_unique_hosts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd14937-1eec-49da-9778-eb7b385da531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of Daily Requests per Host\n",
    "daily_hosts_dataframe = (host_per_day_distinct_dataframe\n",
    "                     .groupBy('day')\n",
    "                     .count()\n",
    "                     .select(col(\"day\"), \n",
    "                                      col(\"count\").alias(\"total_hosts\")))\n",
    "\n",
    "total_daily_reqests_dataframe = (logs_dataframe\n",
    "                              .select(SparkSqlFunctions.dayofmonth(\"time\")\n",
    "                                          .alias(\"day\"))\n",
    "                              .groupBy(\"day\")\n",
    "                              .count()\n",
    "                              .select(col(\"day\"), \n",
    "                                      col(\"count\").alias(\"total_reqs\")))\n",
    "\n",
    "avg_daily_reqests_per_host_dataframe = total_daily_reqests_dataframe.join(daily_hosts_dataframe, 'day')\n",
    "avg_daily_reqests_per_host_dataframe = (avg_daily_reqests_per_host_dataframe\n",
    "                                    .withColumn('avg_reqs', col('total_reqs') / col('total_hosts'))\n",
    "                                    .sort(\"day\"))\n",
    "avg_daily_reqests_per_host_dataframe = avg_daily_reqests_per_host_dataframe.toPandas()\n",
    "avg_daily_reqests_per_host_dataframe\n",
    "\n",
    "avg_daily_reqests_per_host_dataframe.to_json('analysis/avg_daily_reqests_per_host.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36332a-77dd-4d07-8401-1e61323b8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting 404 Response Codes\n",
    "not_found_dataframe = logs_dataframe.filter(logs_dataframe[\"status\"] == 404).cache()\n",
    "print(('Total 404 responses: {}').format(not_found_dataframe.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44706b24-0e60-414f-a002-4e0164c9d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the Top Twenty 404 Response Code Endpoints\n",
    "endpoints_404_count_dataframe = (not_found_dataframe\n",
    "                          .groupBy(\"endpoint\")\n",
    "                          .count()\n",
    "                          .sort(\"count\", ascending=False)\n",
    "                          .limit(20))\n",
    "\n",
    "endpoints_404_count_dataframe.show(truncate=False)\n",
    "\n",
    "endpoints_404_count_dataframe = endpoints_404_count_dataframe.toPandas()\n",
    "\n",
    "endpoints_404_count_dataframe.to_json('analysis/endpoints_404_count.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1b514-6904-401e-9fea-c325dfdd09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the Top Twenty 404 Response Code Hosts\n",
    "\n",
    "hosts_404_count_dataframe = (not_found_dataframe\n",
    "                          .groupBy(\"host\")\n",
    "                          .count()\n",
    "                          .sort(\"count\", ascending=False)\n",
    "                          .limit(20))\n",
    "\n",
    "hosts_404_count_dataframe.show(truncate=False)\n",
    "\n",
    "hosts_404_count_dataframe = hosts_404_count_dataframe.toPandas()\n",
    "\n",
    "hosts_404_count_dataframe.to_json('analysis/hosts_404_count.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6edb7-7e47-4191-bbaf-eb0d4166f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing 404 Errors per Day\n",
    "errors_by_date_sorted_dataframe = (not_found_dataframe\n",
    "                                .groupBy(SparkSqlFunctions.dayofmonth('time').alias('day'))\n",
    "                                .count()\n",
    "                                .sort(\"day\"))\n",
    "\n",
    "errors_by_date_sorted_pd_dataframe = errors_by_date_sorted_dataframe.toPandas()\n",
    "errors_by_date_sorted_pd_dataframe.to_json('analysis/404_errors_by_date_sorted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27bbdb-2fef-46df-93ed-a60cc63e7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 Days for 404 Errors\n",
    "\n",
    "(errors_by_date_sorted_dataframe\n",
    "    .sort(\"count\", ascending=False)\n",
    "    .show(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c578e1-668e-4f41-992a-af8c63b3967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Hourly 404 Errors\n",
    "hourly_avg_errors_sorted_dataframe = (not_found_dataframe\n",
    "                                   .groupBy(SparkSqlFunctions.hour('time')\n",
    "                                             .alias('hour'))\n",
    "                                   .count()\n",
    "                                   .sort('hour'))\n",
    "hourly_avg_errors_sorted_pd_dataframe = hourly_avg_errors_sorted_dataframe.toPandas()\n",
    "\n",
    "hourly_avg_errors_sorted_pd_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f18896-57a1-4e18-94ac-b49682ab9539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
